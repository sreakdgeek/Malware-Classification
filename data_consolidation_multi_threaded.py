# -*- coding: utf-8 -*-

"""
@author: Srikanth Vidapanakal
"""

from multiprocessing import Pool
import os
import gzip
from csv import writer
import six
from threading import Thread
import Queue

read_mode, write_mode = ('r','w') if six.PY2 else ('rt','wt')

path = '/root/hackathon/' #Path to project 
os.chdir(path)

if six.PY2:
    from itertools import izip
    zp = izip
else:
    zp = zip

# Give path to gzip of asm files
#paths = ['train','test']
paths = ['train']

# parse each file preferably in a separate thread
def parse_one_file(q):
	fname = q.get()
	print "got task for fname = %s" % fname
	
        row_file = fname + '.row'
        wf = open(row_file, 'w')
        wf1 = writer(wf)

        # Creating row set
        my_row = []

        f = gzip.open(fname, read_mode)
        twoByte = [0]*16**2
        no_que_mark = 0

	try: 
        	for row in f:
           		codes = row[:-2].split()[1:]

           		# Finding number of times ?? appears
           		no_que_mark += codes.count('??')

           		# Conversion of code to to two byte
           		twoByteCode = [int(i,16) for i in codes if i != '??']

           		# Frequency calculation of two byte codes
           		for i in twoByteCode:
               			twoByte[i] += 1

        	# Row added
        	my_row.append([fname[:fname.find('.bytes.gz')], no_que_mark] + twoByte)
        	wf1.writerow(my_row)
		print "Task done"
		q.task_done()
		print q.qsize()
	except:
		print fname
		pass
		q.task_done()

def consolidate(path):

    ''' A consolidation of given train or test files
        This function reads each asm files (stored in gzip format)
        and prepare summary. asm gzip files are stored in train_gz 
        and test_gz locating.
    '''

    s_path = path + '_gz/'
    Files = os.listdir(s_path)
    byteFiles = [i for i in Files if '.bytes.gz' in i]
    consolidatedFile = path + '_consolidation.gz'
    num_threads = 5

    with gzip.open(consolidatedFile, write_mode) as f:

        # Preparing header part
        fw = writer(f)

        colnames = ['filename', 'no_que_mark']
        colnames += ['TB_'+hex(i)[2:] for i in range(16**2)]
        colnames += ['FB_'+hex(i)[2:] for i in range(16**4)]

        fw.writerow(colnames)
        q = Queue.Queue(maxsize=0)

        for i in range(num_threads):
		worker = Thread(target=parse_one_file, args=(q,)) 
		worker.setDaemon(True)
		worker.start()

	count = 0
        for t, fname in enumerate(byteFiles):
                name = s_path + fname
		q.put(name)
		count += 1
		print q.qsize()

        q.join()

    del Files, byteFiles, colnames, s_path, consolidation, f, fw, twoByte, twoByteCode, consolidatedFile


if __name__ == '__main__':

    p = Pool(1)
    p.map(consolidate, paths)
