# -*- coding: utf-8 -*-

"""
@author: Srikanth Vidapanakal
"""

import os
import gzip
from csv import writer
import six
#from threading import Thread
from multiprocessing import Pool, JoinableQueue, Queue
import logging

read_mode, write_mode = ('r','w') if six.PY2 else ('rt','wt')
path = '/root/hackathon/' #Path to project 
os.chdir(path)

logging.basicConfig(filename='/root/hackathon/data_consolidation.log',
   			    filemode='a',
                            format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',
                            datefmt='%H:%M:%S',
                            level=logging.DEBUG)

if six.PY2:
    from itertools import izip
    zp = izip
else:
    zp = zip

# Give path to gzip of asm files
#paths = ['train','test']
paths = 'train'

# parse each file preferably in a separate thread
def parse_one_file((q_in, q_out)):
	while True:
		fname = q_in.get()
		#print "got task for fname = %s" % fname
	
        	# Creating row set
        	my_row = []

        	f = gzip.open(fname, read_mode)
        	twoByte = [0]*16**2
        	no_que_mark = 0

		try: 
        		for row in f:
           			codes = row[:-2].split()[1:]

           			# Finding number of times ?? appears
           			no_que_mark += codes.count('??')

           			# Conversion of code to to two byte
           			twoByteCode = [int(i,16) for i in codes if i != '??']

           			# Frequency calculation of two byte codes
           			for i in twoByteCode:
               				twoByte[i] += 1

        		# Row added
        		my_row.append([fname[:fname.find('.bytes.gz')], no_que_mark] + twoByte)
			q_out.put(my_row)
			logging.debug('Task done')
			q_in.task_done()
		except:
			logging.debug('Bad file encountered')
			logging.debug(fname)
			pass
			q_in.task_done()

if __name__ == '__main__':

    	s_path = path + paths + '_gz/'
    	Files = os.listdir(s_path)
    	byteFiles = [i for i in Files if '.bytes.gz' in i]

        q_in = JoinableQueue(50)
	q_out = Queue()

	pool = Pool(40, parse_one_file, ((q_in, q_out),))

	count = 0
        for t, fname in enumerate(byteFiles):
                name = s_path + fname
		q_in.put(name)

	q_in.close()
        q_in.join()
	pool.close()

    	consolidatedFile = paths + '_consolidation.csv'
	wf = open(consolidatedFile, 'w')
	wf1 = writer(wf)

	colnames = ['filename', 'no_que_mark']
	colnames += ['TB_'+hex(i)[2:] for i in range(16**2)]
	colnames += ['FB_'+hex(i)[2:] for i in range(16**4)]

	wf1.writerow(colnames)

	for i in range(q_out.qsize()):
		wf1.writerow(q_out.get())
